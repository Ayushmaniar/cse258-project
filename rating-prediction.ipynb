{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":77759,"sourceType":"datasetVersion","datasetId":339}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:09:34.200599Z","iopub.execute_input":"2024-11-30T22:09:34.200832Z","iopub.status.idle":"2024-11-30T22:09:36.893739Z","shell.execute_reply.started":"2024-11-30T22:09:34.200808Z","shell.execute_reply":"2024-11-30T22:09:36.893006Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"ratings = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv', \n                      parse_dates=['timestamp'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:09:36.895617Z","iopub.execute_input":"2024-11-30T22:09:36.896023Z","iopub.status.idle":"2024-11-30T22:10:27.026164Z","shell.execute_reply.started":"2024-11-30T22:09:36.895996Z","shell.execute_reply":"2024-11-30T22:10:27.025166Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"rand_userIds = np.random.choice(ratings['userId'].unique(), \n                                size=int(len(ratings['userId'].unique())*0.3), \n                                replace=False)\n\nratings = ratings.loc[ratings['userId'].isin(rand_userIds)]\n\nprint('There are {} rows of data from {} users'.format(len(ratings), len(rand_userIds)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:27.027668Z","iopub.execute_input":"2024-11-30T22:10:27.027943Z","iopub.status.idle":"2024-11-30T22:10:27.546697Z","shell.execute_reply.started":"2024-11-30T22:10:27.027918Z","shell.execute_reply":"2024-11-30T22:10:27.545049Z"}},"outputs":[{"name":"stdout","text":"There are 6054025 rows of data from 41547 users\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n                                .rank(method='first', ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:27.548324Z","iopub.execute_input":"2024-11-30T22:10:27.550341Z","iopub.status.idle":"2024-11-30T22:10:28.569428Z","shell.execute_reply.started":"2024-11-30T22:10:27.550279Z","shell.execute_reply":"2024-11-30T22:10:28.568501Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_ratings = ratings[ratings['rank_latest'] != 1]\ntest_ratings = ratings[ratings['rank_latest'] == 1]\n\n# drop columns that we no longer need\ntrain_ratings = train_ratings[['userId', 'movieId', 'rating']]\ntest_ratings = test_ratings[['userId', 'movieId', 'rating']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.570560Z","iopub.execute_input":"2024-11-30T22:10:28.570806Z","iopub.status.idle":"2024-11-30T22:10:28.846791Z","shell.execute_reply.started":"2024-11-30T22:10:28.570783Z","shell.execute_reply":"2024-11-30T22:10:28.846038Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from collections import defaultdict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.847863Z","iopub.execute_input":"2024-11-30T22:10:28.848114Z","iopub.status.idle":"2024-11-30T22:10:28.852160Z","shell.execute_reply.started":"2024-11-30T22:10:28.848089Z","shell.execute_reply":"2024-11-30T22:10:28.851360Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ordered_dict_of_all_movies = []\n# ordered_dict_of_all_users = []\n# ratings = defaultdict(int)\n\n# for _,row in train_ratings.iterrows():\n#     u,m,r = row\n#     ordered_dict_of_all_movies.append(m)\n#     ordered_dict_of_all_users.append(u)\n#     ratings[(u,m)] = r\n\n# ordered_dict_of_all_movies = list(set(ordered_dict_of_all_movies))\n# ordered_dict_of_all_movies.sort()\n# ordered_dict_of_all_users = list(set(ordered_dict_of_all_users))\n# ordered_dict_of_all_users.sort()\n\n# print(f'Unique users : {len(ordered_dict_of_all_users)}')\n# print(f'Unique movies : {len(ordered_dict_of_all_movies)}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.854911Z","iopub.execute_input":"2024-11-30T22:10:28.855186Z","iopub.status.idle":"2024-11-30T22:10:28.863979Z","shell.execute_reply.started":"2024-11-30T22:10:28.855159Z","shell.execute_reply":"2024-11-30T22:10:28.863083Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import defaultdict\nimport numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.877787Z","iopub.execute_input":"2024-11-30T22:10:28.878019Z","iopub.status.idle":"2024-11-30T22:10:28.891173Z","shell.execute_reply.started":"2024-11-30T22:10:28.877996Z","shell.execute_reply":"2024-11-30T22:10:28.890463Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"latent_dim = 32\n\n# gamma_user = numpy.random.random((len(ordered_dict_of_all_users),latent_dim))\n# gamma_movies = numpy.random.random((len(ordered_dict_of_all_movies),latent_dim))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.892080Z","iopub.execute_input":"2024-11-30T22:10:28.892354Z","iopub.status.idle":"2024-11-30T22:10:28.903953Z","shell.execute_reply.started":"2024-11-30T22:10:28.892318Z","shell.execute_reply":"2024-11-30T22:10:28.903337Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.904841Z","iopub.execute_input":"2024-11-30T22:10:28.905062Z","iopub.status.idle":"2024-11-30T22:10:28.944385Z","shell.execute_reply.started":"2024-11-30T22:10:28.905039Z","shell.execute_reply":"2024-11-30T22:10:28.943535Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class RatingPredictor(nn.Module):\n    def __init__(self, n_users, n_movies,latent_dim = 32):\n        super(RatingPredictor, self).__init__()\n        \n        # Properly register parameters using nn.Parameter and move to device\n        self.alpha = nn.Parameter(torch.tensor([3.0], device=device))\n        self.beta_user = nn.Parameter(torch.randn(n_users, 1, device=device))\n        self.beta_movies = nn.Parameter(torch.randn(n_movies, 1, device=device))\n        self.gamma_user = nn.Parameter(torch.randn(n_users, latent_dim, device=device))\n        self.gamma_movies = nn.Parameter(torch.randn(n_movies, latent_dim, device=device))\n        \n        # Initialize parameters with normal distribution\n        with torch.no_grad():\n            nn.init.normal_(self.beta_user, std=0.01)\n            nn.init.normal_(self.beta_movies, std=0.01)\n            nn.init.normal_(self.gamma_user, std = 1/np.sqrt(latent_dim))\n            nn.init.normal_(self.gamma_movies, std = 1/np.sqrt(latent_dim))\n\n    def forward(self, user_indices, movie_indices):\n        user_biases = self.beta_user[user_indices]\n        movie_biases = self.beta_movies[movie_indices]\n\n        user_preferences = self.gamma_user[user_indices]\n        movies_preferences = self.gamma_movies[movie_indices]\n\n        interaction = torch.sum(user_preferences * movies_preferences, dim=1)\n        predictions = self.alpha + user_biases + movie_biases + interaction\n        return predictions.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.945482Z","iopub.execute_input":"2024-11-30T22:10:28.945710Z","iopub.status.idle":"2024-11-30T22:10:28.956624Z","shell.execute_reply.started":"2024-11-30T22:10:28.945688Z","shell.execute_reply":"2024-11-30T22:10:28.955860Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def prepare_data(ratingsTrain):\n    \"\"\"\n    Prepare data for PyTorch training\n    \"\"\"\n    # Get unique users and movies\n    users = np.array([u for u, _, _ in ratingsTrain])\n    movies = np.array([b for _, b, _ in ratingsTrain])\n    ratings = np.array([r for _, _, r in ratingsTrain])\n    \n    # Get unique sorted arrays\n    ordered_dict_of_all_users = np.unique(users)\n    ordered_dict_of_all_movies = np.unique(movies)\n    \n    # Create mapping dictionaries\n    user_to_idx = {u: i for i, u in enumerate(ordered_dict_of_all_users)}\n    movie_to_idx = {b: i for i, b in enumerate(ordered_dict_of_all_movies)}\n    \n    # Convert to indices\n    user_indices = np.array([user_to_idx[u] for u in users])\n    movie_indices = np.array([movie_to_idx[b] for b in movies])\n    \n    # Convert to PyTorch tensors\n    user_indices = torch.LongTensor(user_indices).to(device)\n    movie_indices = torch.LongTensor(movie_indices).to(device)\n    ratings = torch.FloatTensor(ratings).to(device)\n    \n    return (user_indices, movie_indices, ratings, \n            user_to_idx, movie_to_idx, \n            len(ordered_dict_of_all_users), len(ordered_dict_of_all_movies))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.957561Z","iopub.execute_input":"2024-11-30T22:10:28.957792Z","iopub.status.idle":"2024-11-30T22:10:28.970008Z","shell.execute_reply.started":"2024-11-30T22:10:28.957769Z","shell.execute_reply":"2024-11-30T22:10:28.969261Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\nclass RatingsDataset(Dataset):\n    def __init__(self, ratings, user_to_idx, movie_to_idx):\n        \"\"\"\n        Initialize the dataset with ratings and mapping dictionaries.\n        \"\"\"\n        self.user_indices = torch.LongTensor([user_to_idx[u] for u, _, _ in ratings])\n        self.movie_indices = torch.LongTensor([movie_to_idx[m] for _, m, _ in ratings])\n        self.ratings = torch.FloatTensor([r for _, _, r in ratings])\n    \n    def __len__(self):\n        return len(self.ratings)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve a single sample from the dataset.\n        \"\"\"\n        return self.user_indices[idx], self.movie_indices[idx], self.ratings[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.970904Z","iopub.execute_input":"2024-11-30T22:10:28.971111Z","iopub.status.idle":"2024-11-30T22:10:28.988794Z","shell.execute_reply.started":"2024-11-30T22:10:28.971090Z","shell.execute_reply":"2024-11-30T22:10:28.988077Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train_model(ratingsTrain, n_epochs=5, lambda_reg=1.0, learning_rate=0.01, batch_size=4096,latent_dim = 32):\n    # Prepare data\n    users = np.array([u for u, _, _ in ratingsTrain])\n    movies = np.array([b for _, b, _ in ratingsTrain])\n    ratings = np.array([r for _, _, r in ratingsTrain])\n\n    print(f'Unique users : {len(np.unique(users))}')\n    \n    \n    # Get unique sorted arrays\n    ordered_dict_of_all_users = np.unique(users)\n    ordered_dict_of_all_movies = np.unique(movies)\n\n    print('Ordered sets created')\n    \n    # Create mapping dictionaries\n    user_to_idx = {u: i for i, u in enumerate(ordered_dict_of_all_users)}\n    movie_to_idx = {b: i for i, b in enumerate(ordered_dict_of_all_movies)}\n\n    print('Mapping dictionaries created')\n    \n    # Create dataset and data loader\n    dataset = RatingsDataset(ratingsTrain, user_to_idx, movie_to_idx)\n\n    print('Dataset created')\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    print('Data loader created')\n\n    \n    # Create model and move to device\n    model = RatingPredictor(len(ordered_dict_of_all_users), len(ordered_dict_of_all_movies), latent_dim).to(device)\n\n    print('Model initialized and moved to device')\n    print(f\"Model parameters: {[p.shape for p in model.parameters()]}\")  # Debug print\n    \n    # Define loss function and optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Verify optimizer has parameters\n    if len(list(model.parameters())) == 0:\n        raise ValueError(\"Model has no parameters!\")\n    \n    # Training loop\n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        for batch_idx, (user_batch, movie_batch, rating_batch) in enumerate(data_loader):\n            user_batch = user_batch.to(device)\n            movie_batch = movie_batch.to(device)\n            rating_batch = rating_batch.to(device)\n            \n            optimizer.zero_grad()\n            predictions = model(user_batch, movie_batch)\n            sse_loss = torch.sum((predictions - rating_batch) ** 2)\n            l2_reg = lambda_reg * (\n                torch.sum(model.beta_user**2) +\n                torch.sum(model.beta_movies**2) +\n                torch.sum(model.gamma_user**2) +\n                torch.sum(model.gamma_movies**2)\n            )\n            loss = sse_loss + l2_reg\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n\n            if batch_idx % 100 == 0:\n                print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()/len(user_batch):.4f}\")\n        \n        if epoch % 1 == 0:\n            print(f'Epoch {epoch}, Loss: {epoch_loss/len(ratingsTrain):.4f}')\n    \n    return model, user_to_idx, movie_to_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:28.989952Z","iopub.execute_input":"2024-11-30T22:10:28.990467Z","iopub.status.idle":"2024-11-30T22:10:29.003481Z","shell.execute_reply.started":"2024-11-30T22:10:28.990429Z","shell.execute_reply":"2024-11-30T22:10:29.002649Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def predict_rating(model, user, movie, user_to_idx, movie_to_idx):\n    \"\"\"\n    Predict rating for a given user-movie pair\n    \"\"\"\n    try:\n        user_idx = torch.LongTensor([user_to_idx[user]]).to(device)\n        movie_idx = torch.LongTensor([movie_to_idx[movie]]).to(device)\n        \n        with torch.no_grad():\n            prediction = model(user_idx, movie_idx)\n        \n        return prediction.item()\n    except:\n        return model.alpha.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:29.004458Z","iopub.execute_input":"2024-11-30T22:10:29.004709Z","iopub.status.idle":"2024-11-30T22:10:29.021951Z","shell.execute_reply.started":"2024-11-30T22:10:29.004685Z","shell.execute_reply":"2024-11-30T22:10:29.021229Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"ratingsTrain = train_ratings.values.tolist()\nratingsValid = test_ratings.values.tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:29.022728Z","iopub.execute_input":"2024-11-30T22:10:29.022951Z","iopub.status.idle":"2024-11-30T22:10:33.943525Z","shell.execute_reply.started":"2024-11-30T22:10:29.022913Z","shell.execute_reply":"2024-11-30T22:10:33.942789Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model, user_to_idx, movie_to_idx = train_model(ratingsTrain ,n_epochs=10, lambda_reg=2.0, learning_rate=0.005,latent_dim = 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:10:33.944524Z","iopub.execute_input":"2024-11-30T22:10:33.944833Z","iopub.status.idle":"2024-11-30T22:22:15.945662Z","shell.execute_reply.started":"2024-11-30T22:10:33.944805Z","shell.execute_reply":"2024-11-30T22:22:15.944747Z"}},"outputs":[{"name":"stdout","text":"Unique users : 41547\nOrdered sets created\nMapping dictionaries created\nDataset created\nData loader created\nModel initialized and moved to device\nModel parameters: [torch.Size([1]), torch.Size([41547, 1]), torch.Size([21707, 1]), torch.Size([41547, 128]), torch.Size([21707, 128])]\nEpoch 0, Batch 0, Loss: 5819.6270\nEpoch 0, Batch 100, Loss: 4467.9072\nEpoch 0, Batch 200, Loss: 4238.9248\nEpoch 0, Batch 300, Loss: 3342.1125\nEpoch 0, Batch 400, Loss: 3110.7031\nEpoch 0, Batch 500, Loss: 3303.4419\nEpoch 0, Batch 600, Loss: 3073.1208\nEpoch 0, Batch 700, Loss: 3035.5696\nEpoch 0, Batch 800, Loss: 2965.3176\nEpoch 0, Batch 900, Loss: 2891.3103\nEpoch 0, Batch 1000, Loss: 2984.1079\nEpoch 0, Batch 1100, Loss: 2857.4832\nEpoch 0, Batch 1200, Loss: 2949.9280\nEpoch 0, Batch 1300, Loss: 2864.2766\nEpoch 0, Batch 1400, Loss: 2774.6577\nEpoch 0, Loss: 3283.4659\nEpoch 1, Batch 0, Loss: 2054.8208\nEpoch 1, Batch 100, Loss: 2041.6888\nEpoch 1, Batch 200, Loss: 1932.9877\nEpoch 1, Batch 300, Loss: 2096.6670\nEpoch 1, Batch 400, Loss: 2193.9314\nEpoch 1, Batch 500, Loss: 2243.2393\nEpoch 1, Batch 600, Loss: 2182.6682\nEpoch 1, Batch 700, Loss: 2280.3530\nEpoch 1, Batch 800, Loss: 2272.0791\nEpoch 1, Batch 900, Loss: 2334.7671\nEpoch 1, Batch 1000, Loss: 2403.3115\nEpoch 1, Batch 1100, Loss: 2321.9324\nEpoch 1, Batch 1200, Loss: 2358.6099\nEpoch 1, Batch 1300, Loss: 2415.4248\nEpoch 1, Batch 1400, Loss: 2479.8496\nEpoch 1, Loss: 2246.4661\nEpoch 2, Batch 0, Loss: 1519.6001\nEpoch 2, Batch 100, Loss: 1564.6829\nEpoch 2, Batch 200, Loss: 1579.4849\nEpoch 2, Batch 300, Loss: 1685.0911\nEpoch 2, Batch 400, Loss: 1649.5669\nEpoch 2, Batch 500, Loss: 1815.0593\nEpoch 2, Batch 600, Loss: 1736.2883\nEpoch 2, Batch 700, Loss: 1856.7502\nEpoch 2, Batch 800, Loss: 1751.2957\nEpoch 2, Batch 900, Loss: 1994.0811\nEpoch 2, Batch 1000, Loss: 1912.1947\nEpoch 2, Batch 1100, Loss: 2019.4105\nEpoch 2, Batch 1200, Loss: 2070.5444\nEpoch 2, Batch 1300, Loss: 2032.3588\nEpoch 2, Batch 1400, Loss: 2053.4177\nEpoch 2, Loss: 1826.5238\nEpoch 3, Batch 0, Loss: 1426.4459\nEpoch 3, Batch 100, Loss: 1445.3429\nEpoch 3, Batch 200, Loss: 1450.1390\nEpoch 3, Batch 300, Loss: 1532.3345\nEpoch 3, Batch 400, Loss: 1507.2240\nEpoch 3, Batch 500, Loss: 1601.7917\nEpoch 3, Batch 600, Loss: 1648.3929\nEpoch 3, Batch 700, Loss: 1629.6188\nEpoch 3, Batch 800, Loss: 1727.9427\nEpoch 3, Batch 900, Loss: 1796.4292\nEpoch 3, Batch 1000, Loss: 1809.5310\nEpoch 3, Batch 1100, Loss: 1810.0254\nEpoch 3, Batch 1200, Loss: 1912.7450\nEpoch 3, Batch 1300, Loss: 1951.0496\nEpoch 3, Batch 1400, Loss: 1973.7312\nEpoch 3, Loss: 1700.2009\nEpoch 4, Batch 0, Loss: 1417.3062\nEpoch 4, Batch 100, Loss: 1391.8848\nEpoch 4, Batch 200, Loss: 1432.0935\nEpoch 4, Batch 300, Loss: 1503.9951\nEpoch 4, Batch 400, Loss: 1544.1173\nEpoch 4, Batch 500, Loss: 1567.5930\nEpoch 4, Batch 600, Loss: 1566.7260\nEpoch 4, Batch 700, Loss: 1564.6992\nEpoch 4, Batch 800, Loss: 1639.7993\nEpoch 4, Batch 900, Loss: 1715.8398\nEpoch 4, Batch 1000, Loss: 1829.9535\nEpoch 4, Batch 1100, Loss: 1831.1208\nEpoch 4, Batch 1200, Loss: 1859.1199\nEpoch 4, Batch 1300, Loss: 1843.7651\nEpoch 4, Batch 1400, Loss: 1967.9824\nEpoch 4, Loss: 1654.9936\nEpoch 5, Batch 0, Loss: 1373.4636\nEpoch 5, Batch 100, Loss: 1418.8508\nEpoch 5, Batch 200, Loss: 1451.2244\nEpoch 5, Batch 300, Loss: 1482.7086\nEpoch 5, Batch 400, Loss: 1473.1433\nEpoch 5, Batch 500, Loss: 1574.1973\nEpoch 5, Batch 600, Loss: 1583.2548\nEpoch 5, Batch 700, Loss: 1586.8972\nEpoch 5, Batch 800, Loss: 1676.8898\nEpoch 5, Batch 900, Loss: 1673.2533\nEpoch 5, Batch 1000, Loss: 1786.6705\nEpoch 5, Batch 1100, Loss: 1811.1541\nEpoch 5, Batch 1200, Loss: 1836.4467\nEpoch 5, Batch 1300, Loss: 1778.5542\nEpoch 5, Batch 1400, Loss: 1826.5945\nEpoch 5, Loss: 1624.1982\nEpoch 6, Batch 0, Loss: 1368.3446\nEpoch 6, Batch 100, Loss: 1358.4806\nEpoch 6, Batch 200, Loss: 1363.9585\nEpoch 6, Batch 300, Loss: 1415.1803\nEpoch 6, Batch 400, Loss: 1526.6693\nEpoch 6, Batch 500, Loss: 1530.9088\nEpoch 6, Batch 600, Loss: 1566.0173\nEpoch 6, Batch 700, Loss: 1621.8551\nEpoch 6, Batch 800, Loss: 1592.6951\nEpoch 6, Batch 900, Loss: 1680.9062\nEpoch 6, Batch 1000, Loss: 1687.9480\nEpoch 6, Batch 1100, Loss: 1752.1382\nEpoch 6, Batch 1200, Loss: 1863.3018\nEpoch 6, Batch 1300, Loss: 1764.8092\nEpoch 6, Batch 1400, Loss: 1775.9778\nEpoch 6, Loss: 1596.4551\nEpoch 7, Batch 0, Loss: 1388.0833\nEpoch 7, Batch 100, Loss: 1299.1261\nEpoch 7, Batch 200, Loss: 1332.7180\nEpoch 7, Batch 300, Loss: 1411.5594\nEpoch 7, Batch 400, Loss: 1449.3403\nEpoch 7, Batch 500, Loss: 1518.0001\nEpoch 7, Batch 600, Loss: 1602.3397\nEpoch 7, Batch 700, Loss: 1552.9673\nEpoch 7, Batch 800, Loss: 1628.0835\nEpoch 7, Batch 900, Loss: 1706.2460\nEpoch 7, Batch 1000, Loss: 1698.5298\nEpoch 7, Batch 1100, Loss: 1680.4053\nEpoch 7, Batch 1200, Loss: 1735.5165\nEpoch 7, Batch 1300, Loss: 1811.5093\nEpoch 7, Batch 1400, Loss: 1780.3909\nEpoch 7, Loss: 1578.6581\nEpoch 8, Batch 0, Loss: 1398.3345\nEpoch 8, Batch 100, Loss: 1390.6930\nEpoch 8, Batch 200, Loss: 1366.2493\nEpoch 8, Batch 300, Loss: 1349.1060\nEpoch 8, Batch 400, Loss: 1412.6108\nEpoch 8, Batch 500, Loss: 1480.0847\nEpoch 8, Batch 600, Loss: 1566.3643\nEpoch 8, Batch 700, Loss: 1494.0112\nEpoch 8, Batch 800, Loss: 1601.2708\nEpoch 8, Batch 900, Loss: 1622.1782\nEpoch 8, Batch 1000, Loss: 1629.1595\nEpoch 8, Batch 1100, Loss: 1719.0304\nEpoch 8, Batch 1200, Loss: 1674.9750\nEpoch 8, Batch 1300, Loss: 1677.3832\nEpoch 8, Batch 1400, Loss: 1702.7258\nEpoch 8, Loss: 1567.3847\nEpoch 9, Batch 0, Loss: 1358.1957\nEpoch 9, Batch 100, Loss: 1362.8748\nEpoch 9, Batch 200, Loss: 1367.4186\nEpoch 9, Batch 300, Loss: 1388.1204\nEpoch 9, Batch 400, Loss: 1417.5671\nEpoch 9, Batch 500, Loss: 1508.4015\nEpoch 9, Batch 600, Loss: 1445.3069\nEpoch 9, Batch 700, Loss: 1556.1875\nEpoch 9, Batch 800, Loss: 1537.5466\nEpoch 9, Batch 900, Loss: 1647.4141\nEpoch 9, Batch 1000, Loss: 1683.5176\nEpoch 9, Batch 1100, Loss: 1621.3418\nEpoch 9, Batch 1200, Loss: 1713.4492\nEpoch 9, Batch 1300, Loss: 1654.4607\nEpoch 9, Batch 1400, Loss: 1786.3701\nEpoch 9, Loss: 1555.9936\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# The model parameters can be accessed as:\nalpha = model.alpha.item()\nbeta_user = model.beta_user.detach().cpu().numpy()\nbeta_movies = model.beta_movies.detach().cpu().numpy()\n\nvalidMSE = 0\n\nfor u,b,r in tqdm(ratingsValid):\n    prediction = predict_rating(model,u,b,user_to_idx, movie_to_idx)\n    validMSE += (r - prediction)**2\n\nvalidMSE /= len(ratingsValid)\nprint(validMSE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:22:15.948937Z","iopub.execute_input":"2024-11-30T22:22:15.949423Z","iopub.status.idle":"2024-11-30T22:22:25.002190Z","shell.execute_reply.started":"2024-11-30T22:22:15.949393Z","shell.execute_reply":"2024-11-30T22:22:25.001386Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 41547/41547 [00:09<00:00, 4594.78it/s]","output_type":"stream"},{"name":"stdout","text":"1.0130723440501455\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}