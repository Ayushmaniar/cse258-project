{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "749a53b6",
   "metadata": {
    "papermill": {
     "duration": 0.00313,
     "end_time": "2024-11-30T03:05:58.672013",
     "exception": false,
     "start_time": "2024-11-30T03:05:58.668883",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# First lets handle all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cfc70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T03:05:58.678427Z",
     "iopub.status.busy": "2024-11-30T03:05:58.678065Z",
     "iopub.status.idle": "2024-11-30T03:06:07.609602Z",
     "shell.execute_reply": "2024-11-30T03:06:07.608710Z"
    },
    "papermill": {
     "duration": 8.937028,
     "end_time": "2024-11-30T03:06:07.611679",
     "exception": false,
     "start_time": "2024-11-30T03:05:58.674651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b2da3",
   "metadata": {
    "papermill": {
     "duration": 0.002535,
     "end_time": "2024-11-30T03:06:07.616923",
     "exception": false,
     "start_time": "2024-11-30T03:06:07.614388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1b2469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T03:06:07.626357Z",
     "iopub.status.busy": "2024-11-30T03:06:07.625479Z",
     "iopub.status.idle": "2024-11-30T03:07:01.696339Z",
     "shell.execute_reply": "2024-11-30T03:07:01.695391Z"
    },
    "papermill": {
     "duration": 54.077581,
     "end_time": "2024-11-30T03:07:01.698123",
     "exception": false,
     "start_time": "2024-11-30T03:06:07.620542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6043155 rows of data from 41547 users\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100881</th>\n",
       "      <td>707</td>\n",
       "      <td>5377</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19890970</th>\n",
       "      <td>137716</td>\n",
       "      <td>6707</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662180</th>\n",
       "      <td>87502</td>\n",
       "      <td>49220</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8081486</th>\n",
       "      <td>55659</td>\n",
       "      <td>3178</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15072808</th>\n",
       "      <td>104067</td>\n",
       "      <td>3527</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating\n",
       "100881       707     5377     1.0\n",
       "19890970  137716     6707     1.0\n",
       "12662180   87502    49220     1.0\n",
       "8081486    55659     3178     1.0\n",
       "15072808  104067     3527     1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv', \n",
    "                      parse_dates=['timestamp'])\n",
    "\n",
    "rand_userIds = np.random.choice(ratings['userId'].unique(), \n",
    "                                size=int(len(ratings['userId'].unique())*0.3), \n",
    "                                replace=False)\n",
    "\n",
    "ratings = ratings.loc[ratings['userId'].isin(rand_userIds)]\n",
    "\n",
    "print('There are {} rows of data from {} users'.format(len(ratings), len(rand_userIds)))\n",
    "\n",
    "ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'] \\\n",
    "                                .rank(method='first', ascending=False)\n",
    "\n",
    "train_ratings = ratings[ratings['rank_latest'] != 1]\n",
    "test_ratings = ratings[ratings['rank_latest'] == 1]\n",
    "\n",
    "# drop columns that we no longer need\n",
    "train_ratings = train_ratings[['userId', 'movieId', 'rating']]\n",
    "test_ratings = test_ratings[['userId', 'movieId', 'rating']]\n",
    "\n",
    "train_ratings.loc[:, 'rating'] = 1\n",
    "all_movieIds = ratings['movieId'].unique()\n",
    "\n",
    "train_ratings.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081751b",
   "metadata": {
    "papermill": {
     "duration": 0.00252,
     "end_time": "2024-11-30T03:07:01.703423",
     "exception": false,
     "start_time": "2024-11-30T03:07:01.700903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lets work with only 30% of the dataset of ratings for managing the CPU capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0aa031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T03:07:01.711239Z",
     "iopub.status.busy": "2024-11-30T03:07:01.710918Z",
     "iopub.status.idle": "2024-11-30T03:07:01.727827Z",
     "shell.execute_reply": "2024-11-30T03:07:01.726979Z"
    },
    "papermill": {
     "duration": 0.023493,
     "end_time": "2024-11-30T03:07:01.729470",
     "exception": false,
     "start_time": "2024-11-30T03:07:01.705977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecommendationDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for Recommendation System with GPU Support\n",
    "    \n",
    "    Args:\n",
    "        interaction_data (pd.DataFrame): DataFrame containing user-item interactions\n",
    "        all_item_ids (list): List of all unique item IDs\n",
    "        processing_device (torch.device): Device to move tensors (default: cuda if available)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, interaction_data, all_item_ids, processing_device=None):\n",
    "        if processing_device is None:\n",
    "            processing_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.processing_device = processing_device\n",
    "        print('Dataset initialization with device configuration.')\n",
    "        self.user_list, self.item_list, self.rating_list = self._prepare_dataset(interaction_data, all_item_ids)\n",
    "        print('Dataset prepared with users, items, and labels.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_list)\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        return self.user_list[index], self.item_list[index], self.rating_list[index]\n",
    "\n",
    "    def _prepare_dataset(self, interaction_data, all_item_ids):\n",
    "        print('Generating negative samples...')\n",
    "        users, items, labels = [], [], []\n",
    "        positive_pairs = set(zip(interaction_data['userId'], interaction_data['movieId']))\n",
    "        num_negative_samples = 4\n",
    "\n",
    "        for user_id, item_id in tqdm(positive_pairs, leave=False, dynamic_ncols=True, desc=\"Progress\"):\n",
    "            users.append(user_id)\n",
    "            items.append(item_id)\n",
    "            labels.append(1)\n",
    "            for _ in range(num_negative_samples):\n",
    "                negative_item = np.random.choice(all_item_ids)\n",
    "                while (user_id, negative_item) in positive_pairs:\n",
    "                    negative_item = np.random.choice(all_item_ids)\n",
    "                users.append(user_id)\n",
    "                items.append(negative_item)\n",
    "                labels.append(0)\n",
    "        \n",
    "        print('Negative samples generated successfully.')\n",
    "\n",
    "        return (\n",
    "            torch.tensor(users).to(self.processing_device),\n",
    "            torch.tensor(items).to(self.processing_device),\n",
    "            torch.tensor(labels).to(self.processing_device)\n",
    "        )\n",
    "\n",
    "\n",
    "class CollaborativeFiltering(pl.LightningModule):\n",
    "    \"\"\"Collaborative Filtering Model with GPU Support\n",
    "    \n",
    "    Args:\n",
    "        total_users (int): Number of unique users\n",
    "        total_items (int): Number of unique items\n",
    "        training_data (pd.DataFrame): User-item interaction data for training\n",
    "        item_ids (list): List of all unique item IDs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, total_users, total_items, training_data, item_ids):\n",
    "        super().__init__()\n",
    "        print('Model initialization started.')\n",
    "        self.user_embedding_layer = nn.Embedding(num_embeddings=total_users, embedding_dim=8)\n",
    "        self.item_embedding_layer = nn.Embedding(num_embeddings=total_items, embedding_dim=8)\n",
    "        self.dense_layer_1 = nn.Linear(in_features=16, out_features=64)\n",
    "        self.dense_layer_2 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.final_output = nn.Linear(in_features=32, out_features=1)\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(64)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(32)\n",
    "        # self.dropout = nn.Dropout(p=0.2)\n",
    "        self.training_data = training_data\n",
    "        self.item_ids = item_ids\n",
    "        print('Model layers initialized successfully.')\n",
    "\n",
    "    def forward(self, user_inputs, item_inputs):\n",
    "        user_vector = self.user_embedding_layer(user_inputs)\n",
    "        item_vector = self.item_embedding_layer(item_inputs)\n",
    "        combined_vector = torch.cat([user_vector, item_vector], dim=-1)\n",
    "        hidden_output = torch.relu(self.dense_layer_1(combined_vector))\n",
    "        hidden_output = torch.relu(self.dense_layer_2(hidden_output))\n",
    "        # hidden_output = self.dropout(torch.relu(self.batch_norm_1(self.dense_layer_1(combined_vector))))\n",
    "        # hidden_output = self.dropout(torch.relu(self.batch_norm_2(self.dense_layer_2(hidden_output))))\n",
    "        prediction = torch.sigmoid(self.final_output(hidden_output))\n",
    "        return prediction\n",
    "    \n",
    "    def training_step(self, batch_data, batch_idx):\n",
    "        user_inputs, item_inputs, labels = batch_data\n",
    "        predictions = self(user_inputs, item_inputs)\n",
    "        loss_value = nn.BCELoss()(predictions, labels.view(-1, 1).float())\n",
    "        return loss_value\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        processing_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return data.DataLoader(\n",
    "            RecommendationDataset(self.training_data, self.item_ids, processing_device),\n",
    "            batch_size=4096,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "\n",
    "def execute_training(interaction_data, training_subset, max_epochs=5, logging_interval=50):\n",
    "    processing_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    total_users = interaction_data['userId'].max() + 1\n",
    "    total_items = interaction_data['movieId'].max() + 1\n",
    "    unique_item_ids = interaction_data['movieId'].unique()\n",
    "\n",
    "    model = CollaborativeFiltering(total_users, total_items, training_subset, unique_item_ids)\n",
    "    model = model.to(processing_device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_function = nn.BCELoss()\n",
    "\n",
    "    print('Training process initialized.')\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        data_loader = DataLoader(\n",
    "            RecommendationDataset(training_subset, unique_item_ids), \n",
    "            batch_size=4096, \n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        print(f\"Starting Epoch {epoch + 1}/{max_epochs}\")\n",
    "        for batch_idx, (users, items, labels) in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_predictions = model(users, items)\n",
    "            loss_value = loss_function(batch_predictions, labels.view(-1, 1).float())\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss_value.item()\n",
    "\n",
    "            if (batch_idx + 1) % logging_interval == 0:\n",
    "                print(f\"  Batch {batch_idx + 1}, Loss: {loss_value.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Average Loss: {epoch_loss / len(data_loader):.4f}\\n\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de79e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T03:07:01.735630Z",
     "iopub.status.busy": "2024-11-30T03:07:01.735373Z",
     "iopub.status.idle": "2024-11-30T04:38:20.443952Z",
     "shell.execute_reply": "2024-11-30T04:38:20.443069Z"
    },
    "papermill": {
     "duration": 5478.713798,
     "end_time": "2024-11-30T04:38:20.445852",
     "exception": false,
     "start_time": "2024-11-30T03:07:01.732054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialization started.\n",
      "Model layers initialized successfully.\n",
      "Training process initialized.\n",
      "Dataset initialization with device configuration.\n",
      "Generating negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples generated successfully.\n",
      "Dataset prepared with users, items, and labels.\n",
      "Starting Epoch 1/5\n",
      "  Batch 100, Loss: 0.5054\n",
      "  Batch 200, Loss: 0.4781\n",
      "  Batch 300, Loss: 0.4658\n",
      "  Batch 400, Loss: 0.4225\n",
      "  Batch 500, Loss: 0.3677\n",
      "  Batch 600, Loss: 0.3106\n",
      "  Batch 700, Loss: 0.2903\n",
      "  Batch 800, Loss: 0.2777\n",
      "  Batch 900, Loss: 0.2638\n",
      "  Batch 1000, Loss: 0.2607\n",
      "  Batch 1100, Loss: 0.2491\n",
      "  Batch 1200, Loss: 0.2510\n",
      "  Batch 1300, Loss: 0.2461\n",
      "  Batch 1400, Loss: 0.2555\n",
      "  Batch 1500, Loss: 0.2626\n",
      "  Batch 1600, Loss: 0.2314\n",
      "  Batch 1700, Loss: 0.2297\n",
      "  Batch 1800, Loss: 0.2549\n",
      "  Batch 1900, Loss: 0.2441\n",
      "  Batch 2000, Loss: 0.2473\n",
      "  Batch 2100, Loss: 0.2360\n",
      "  Batch 2200, Loss: 0.2327\n",
      "  Batch 2300, Loss: 0.2346\n",
      "  Batch 2400, Loss: 0.2298\n",
      "  Batch 2500, Loss: 0.2246\n",
      "  Batch 2600, Loss: 0.2227\n",
      "  Batch 2700, Loss: 0.2267\n",
      "  Batch 2800, Loss: 0.2342\n",
      "  Batch 2900, Loss: 0.2273\n",
      "  Batch 3000, Loss: 0.2112\n",
      "  Batch 3100, Loss: 0.2346\n",
      "  Batch 3200, Loss: 0.2268\n",
      "  Batch 3300, Loss: 0.2329\n",
      "  Batch 3400, Loss: 0.2392\n",
      "  Batch 3500, Loss: 0.2142\n",
      "  Batch 3600, Loss: 0.2251\n",
      "  Batch 3700, Loss: 0.2328\n",
      "  Batch 3800, Loss: 0.2285\n",
      "  Batch 3900, Loss: 0.2220\n",
      "  Batch 4000, Loss: 0.2335\n",
      "  Batch 4100, Loss: 0.2260\n",
      "  Batch 4200, Loss: 0.2278\n",
      "  Batch 4300, Loss: 0.2227\n",
      "  Batch 4400, Loss: 0.2249\n",
      "  Batch 4500, Loss: 0.2251\n",
      "  Batch 4600, Loss: 0.2334\n",
      "  Batch 4700, Loss: 0.2250\n",
      "  Batch 4800, Loss: 0.2206\n",
      "  Batch 4900, Loss: 0.2296\n",
      "  Batch 5000, Loss: 0.2315\n",
      "  Batch 5100, Loss: 0.2325\n",
      "  Batch 5200, Loss: 0.2238\n",
      "  Batch 5300, Loss: 0.2250\n",
      "  Batch 5400, Loss: 0.2297\n",
      "  Batch 5500, Loss: 0.2231\n",
      "  Batch 5600, Loss: 0.2167\n",
      "  Batch 5700, Loss: 0.2362\n",
      "  Batch 5800, Loss: 0.2235\n",
      "  Batch 5900, Loss: 0.2302\n",
      "  Batch 6000, Loss: 0.2309\n",
      "  Batch 6100, Loss: 0.2210\n",
      "  Batch 6200, Loss: 0.2160\n",
      "  Batch 6300, Loss: 0.2348\n",
      "  Batch 6400, Loss: 0.2258\n",
      "  Batch 6500, Loss: 0.2191\n",
      "  Batch 6600, Loss: 0.2371\n",
      "  Batch 6700, Loss: 0.2116\n",
      "  Batch 6800, Loss: 0.2252\n",
      "  Batch 6900, Loss: 0.2181\n",
      "  Batch 7000, Loss: 0.2281\n",
      "  Batch 7100, Loss: 0.2015\n",
      "  Batch 7200, Loss: 0.2245\n",
      "  Batch 7300, Loss: 0.2187\n",
      "Epoch 1 Average Loss: 0.2504\n",
      "\n",
      "Dataset initialization with device configuration.\n",
      "Generating negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples generated successfully.\n",
      "Dataset prepared with users, items, and labels.\n",
      "Starting Epoch 2/5\n",
      "  Batch 100, Loss: 0.2213\n",
      "  Batch 200, Loss: 0.2085\n",
      "  Batch 300, Loss: 0.2158\n",
      "  Batch 400, Loss: 0.2105\n",
      "  Batch 500, Loss: 0.2260\n",
      "  Batch 600, Loss: 0.2250\n",
      "  Batch 700, Loss: 0.2132\n",
      "  Batch 800, Loss: 0.2184\n",
      "  Batch 900, Loss: 0.2283\n",
      "  Batch 1000, Loss: 0.2135\n",
      "  Batch 1100, Loss: 0.2141\n",
      "  Batch 1200, Loss: 0.2147\n",
      "  Batch 1300, Loss: 0.2207\n",
      "  Batch 1400, Loss: 0.2307\n",
      "  Batch 1500, Loss: 0.2182\n",
      "  Batch 1600, Loss: 0.2269\n",
      "  Batch 1700, Loss: 0.2083\n",
      "  Batch 1800, Loss: 0.2334\n",
      "  Batch 1900, Loss: 0.2145\n",
      "  Batch 2000, Loss: 0.2172\n",
      "  Batch 2100, Loss: 0.2047\n",
      "  Batch 2200, Loss: 0.2055\n",
      "  Batch 2300, Loss: 0.2150\n",
      "  Batch 2400, Loss: 0.2077\n",
      "  Batch 2500, Loss: 0.2235\n",
      "  Batch 2600, Loss: 0.2243\n",
      "  Batch 2700, Loss: 0.2220\n",
      "  Batch 2800, Loss: 0.2111\n",
      "  Batch 2900, Loss: 0.2158\n",
      "  Batch 3000, Loss: 0.2044\n",
      "  Batch 3100, Loss: 0.2153\n",
      "  Batch 3200, Loss: 0.2164\n",
      "  Batch 3300, Loss: 0.2047\n",
      "  Batch 3400, Loss: 0.2122\n",
      "  Batch 3500, Loss: 0.2138\n",
      "  Batch 3600, Loss: 0.2222\n",
      "  Batch 3700, Loss: 0.2079\n",
      "  Batch 3800, Loss: 0.2149\n",
      "  Batch 3900, Loss: 0.2205\n",
      "  Batch 4000, Loss: 0.2120\n",
      "  Batch 4100, Loss: 0.2070\n",
      "  Batch 4200, Loss: 0.2226\n",
      "  Batch 4300, Loss: 0.2110\n",
      "  Batch 4400, Loss: 0.2226\n",
      "  Batch 4500, Loss: 0.2158\n",
      "  Batch 4600, Loss: 0.2255\n",
      "  Batch 4700, Loss: 0.2333\n",
      "  Batch 4800, Loss: 0.2210\n",
      "  Batch 4900, Loss: 0.2188\n",
      "  Batch 5000, Loss: 0.2034\n",
      "  Batch 5100, Loss: 0.2224\n",
      "  Batch 5200, Loss: 0.2064\n",
      "  Batch 5300, Loss: 0.2057\n",
      "  Batch 5400, Loss: 0.2162\n",
      "  Batch 5500, Loss: 0.2159\n",
      "  Batch 5600, Loss: 0.2241\n",
      "  Batch 5700, Loss: 0.2134\n",
      "  Batch 5800, Loss: 0.2222\n",
      "  Batch 5900, Loss: 0.2195\n",
      "  Batch 6000, Loss: 0.2137\n",
      "  Batch 6100, Loss: 0.2296\n",
      "  Batch 6200, Loss: 0.2161\n",
      "  Batch 6300, Loss: 0.2324\n",
      "  Batch 6400, Loss: 0.2249\n",
      "  Batch 6500, Loss: 0.2149\n",
      "  Batch 6600, Loss: 0.2223\n",
      "  Batch 6700, Loss: 0.2163\n",
      "  Batch 6800, Loss: 0.2171\n",
      "  Batch 6900, Loss: 0.2140\n",
      "  Batch 7000, Loss: 0.2197\n",
      "  Batch 7100, Loss: 0.2225\n",
      "  Batch 7200, Loss: 0.2118\n",
      "  Batch 7300, Loss: 0.2225\n",
      "Epoch 2 Average Loss: 0.2181\n",
      "\n",
      "Dataset initialization with device configuration.\n",
      "Generating negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples generated successfully.\n",
      "Dataset prepared with users, items, and labels.\n",
      "Starting Epoch 3/5\n",
      "  Batch 100, Loss: 0.2126\n",
      "  Batch 200, Loss: 0.2224\n",
      "  Batch 300, Loss: 0.2110\n",
      "  Batch 400, Loss: 0.2091\n",
      "  Batch 500, Loss: 0.2091\n",
      "  Batch 600, Loss: 0.2151\n",
      "  Batch 700, Loss: 0.2037\n",
      "  Batch 800, Loss: 0.2224\n",
      "  Batch 900, Loss: 0.2137\n",
      "  Batch 1000, Loss: 0.2196\n",
      "  Batch 1100, Loss: 0.2118\n",
      "  Batch 1200, Loss: 0.2114\n",
      "  Batch 1300, Loss: 0.2143\n",
      "  Batch 1400, Loss: 0.2089\n",
      "  Batch 1500, Loss: 0.2160\n",
      "  Batch 1600, Loss: 0.2058\n",
      "  Batch 1700, Loss: 0.2074\n",
      "  Batch 1800, Loss: 0.2200\n",
      "  Batch 1900, Loss: 0.2175\n",
      "  Batch 2000, Loss: 0.2235\n",
      "  Batch 2100, Loss: 0.2265\n",
      "  Batch 2200, Loss: 0.2156\n",
      "  Batch 2300, Loss: 0.2174\n",
      "  Batch 2400, Loss: 0.2238\n",
      "  Batch 2500, Loss: 0.2117\n",
      "  Batch 2600, Loss: 0.2017\n",
      "  Batch 2700, Loss: 0.2080\n",
      "  Batch 2800, Loss: 0.2211\n",
      "  Batch 2900, Loss: 0.2064\n",
      "  Batch 3000, Loss: 0.2004\n",
      "  Batch 3100, Loss: 0.2108\n",
      "  Batch 3200, Loss: 0.2138\n",
      "  Batch 3300, Loss: 0.2109\n",
      "  Batch 3400, Loss: 0.2058\n",
      "  Batch 3500, Loss: 0.2111\n",
      "  Batch 3600, Loss: 0.2131\n",
      "  Batch 3700, Loss: 0.2088\n",
      "  Batch 3800, Loss: 0.2100\n",
      "  Batch 3900, Loss: 0.2244\n",
      "  Batch 4000, Loss: 0.2178\n",
      "  Batch 4100, Loss: 0.2168\n",
      "  Batch 4200, Loss: 0.2183\n",
      "  Batch 4300, Loss: 0.2245\n",
      "  Batch 4400, Loss: 0.2162\n",
      "  Batch 4500, Loss: 0.2042\n",
      "  Batch 4600, Loss: 0.2151\n",
      "  Batch 4700, Loss: 0.2189\n",
      "  Batch 4800, Loss: 0.2079\n",
      "  Batch 4900, Loss: 0.2158\n",
      "  Batch 5000, Loss: 0.2179\n",
      "  Batch 5100, Loss: 0.2131\n",
      "  Batch 5200, Loss: 0.2126\n",
      "  Batch 5300, Loss: 0.1996\n",
      "  Batch 5400, Loss: 0.2228\n",
      "  Batch 5500, Loss: 0.2161\n",
      "  Batch 5600, Loss: 0.2005\n",
      "  Batch 5700, Loss: 0.2136\n",
      "  Batch 5800, Loss: 0.2176\n",
      "  Batch 5900, Loss: 0.2146\n",
      "  Batch 6000, Loss: 0.2091\n",
      "  Batch 6100, Loss: 0.1977\n",
      "  Batch 6200, Loss: 0.2204\n",
      "  Batch 6300, Loss: 0.2089\n",
      "  Batch 6400, Loss: 0.2133\n",
      "  Batch 6500, Loss: 0.2147\n",
      "  Batch 6600, Loss: 0.2097\n",
      "  Batch 6700, Loss: 0.2077\n",
      "  Batch 6800, Loss: 0.2170\n",
      "  Batch 6900, Loss: 0.2067\n",
      "  Batch 7000, Loss: 0.2134\n",
      "  Batch 7100, Loss: 0.2182\n",
      "  Batch 7200, Loss: 0.2030\n",
      "  Batch 7300, Loss: 0.2328\n",
      "Epoch 3 Average Loss: 0.2139\n",
      "\n",
      "Dataset initialization with device configuration.\n",
      "Generating negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples generated successfully.\n",
      "Dataset prepared with users, items, and labels.\n",
      "Starting Epoch 4/5\n",
      "  Batch 100, Loss: 0.2141\n",
      "  Batch 200, Loss: 0.2196\n",
      "  Batch 300, Loss: 0.2013\n",
      "  Batch 400, Loss: 0.2164\n",
      "  Batch 500, Loss: 0.2050\n",
      "  Batch 600, Loss: 0.2096\n",
      "  Batch 700, Loss: 0.2147\n",
      "  Batch 800, Loss: 0.2132\n",
      "  Batch 900, Loss: 0.2112\n",
      "  Batch 1000, Loss: 0.2162\n",
      "  Batch 1100, Loss: 0.2148\n",
      "  Batch 1200, Loss: 0.2145\n",
      "  Batch 1300, Loss: 0.1976\n",
      "  Batch 1400, Loss: 0.2066\n",
      "  Batch 1500, Loss: 0.2106\n",
      "  Batch 1600, Loss: 0.2134\n",
      "  Batch 1700, Loss: 0.2013\n",
      "  Batch 1800, Loss: 0.2093\n",
      "  Batch 1900, Loss: 0.2105\n",
      "  Batch 2000, Loss: 0.2053\n",
      "  Batch 2100, Loss: 0.2200\n",
      "  Batch 2200, Loss: 0.2106\n",
      "  Batch 2300, Loss: 0.2167\n",
      "  Batch 2400, Loss: 0.2056\n",
      "  Batch 2500, Loss: 0.1947\n",
      "  Batch 2600, Loss: 0.2080\n",
      "  Batch 2700, Loss: 0.1956\n",
      "  Batch 2800, Loss: 0.2038\n",
      "  Batch 2900, Loss: 0.2116\n",
      "  Batch 3000, Loss: 0.2079\n",
      "  Batch 3100, Loss: 0.2288\n",
      "  Batch 3200, Loss: 0.2064\n",
      "  Batch 3300, Loss: 0.2104\n",
      "  Batch 3400, Loss: 0.2096\n",
      "  Batch 3500, Loss: 0.2150\n",
      "  Batch 3600, Loss: 0.2045\n",
      "  Batch 3700, Loss: 0.2269\n",
      "  Batch 3800, Loss: 0.2116\n",
      "  Batch 3900, Loss: 0.1997\n",
      "  Batch 4000, Loss: 0.2120\n",
      "  Batch 4100, Loss: 0.2050\n",
      "  Batch 4200, Loss: 0.2150\n",
      "  Batch 4300, Loss: 0.2221\n",
      "  Batch 4400, Loss: 0.2244\n",
      "  Batch 4500, Loss: 0.2144\n",
      "  Batch 4600, Loss: 0.2125\n",
      "  Batch 4700, Loss: 0.2047\n",
      "  Batch 4800, Loss: 0.2146\n",
      "  Batch 4900, Loss: 0.2194\n",
      "  Batch 5000, Loss: 0.2180\n",
      "  Batch 5100, Loss: 0.2141\n",
      "  Batch 5200, Loss: 0.2259\n",
      "  Batch 5300, Loss: 0.2169\n",
      "  Batch 5400, Loss: 0.2012\n",
      "  Batch 5500, Loss: 0.1997\n",
      "  Batch 5600, Loss: 0.2028\n",
      "  Batch 5700, Loss: 0.2104\n",
      "  Batch 5800, Loss: 0.1903\n",
      "  Batch 5900, Loss: 0.2142\n",
      "  Batch 6000, Loss: 0.2211\n",
      "  Batch 6100, Loss: 0.2111\n",
      "  Batch 6200, Loss: 0.2120\n",
      "  Batch 6300, Loss: 0.2116\n",
      "  Batch 6400, Loss: 0.2085\n",
      "  Batch 6500, Loss: 0.2195\n",
      "  Batch 6600, Loss: 0.2145\n",
      "  Batch 6700, Loss: 0.2033\n",
      "  Batch 6800, Loss: 0.2192\n",
      "  Batch 6900, Loss: 0.2058\n",
      "  Batch 7000, Loss: 0.2231\n",
      "  Batch 7100, Loss: 0.2034\n",
      "  Batch 7200, Loss: 0.2040\n",
      "  Batch 7300, Loss: 0.2177\n",
      "Epoch 4 Average Loss: 0.2104\n",
      "\n",
      "Dataset initialization with device configuration.\n",
      "Generating negative samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples generated successfully.\n",
      "Dataset prepared with users, items, and labels.\n",
      "Starting Epoch 5/5\n",
      "  Batch 100, Loss: 0.2102\n",
      "  Batch 200, Loss: 0.2216\n",
      "  Batch 300, Loss: 0.2104\n",
      "  Batch 400, Loss: 0.2021\n",
      "  Batch 500, Loss: 0.2082\n",
      "  Batch 600, Loss: 0.2002\n",
      "  Batch 700, Loss: 0.2098\n",
      "  Batch 800, Loss: 0.1946\n",
      "  Batch 900, Loss: 0.2059\n",
      "  Batch 1000, Loss: 0.1905\n",
      "  Batch 1100, Loss: 0.2080\n",
      "  Batch 1200, Loss: 0.2101\n",
      "  Batch 1300, Loss: 0.2127\n",
      "  Batch 1400, Loss: 0.2030\n",
      "  Batch 1500, Loss: 0.2074\n",
      "  Batch 1600, Loss: 0.2091\n",
      "  Batch 1700, Loss: 0.2093\n",
      "  Batch 1800, Loss: 0.2107\n",
      "  Batch 1900, Loss: 0.2117\n",
      "  Batch 2000, Loss: 0.1924\n",
      "  Batch 2100, Loss: 0.2069\n",
      "  Batch 2200, Loss: 0.2074\n",
      "  Batch 2300, Loss: 0.2053\n",
      "  Batch 2400, Loss: 0.2017\n",
      "  Batch 2500, Loss: 0.2171\n",
      "  Batch 2600, Loss: 0.2059\n",
      "  Batch 2700, Loss: 0.1953\n",
      "  Batch 2800, Loss: 0.2072\n",
      "  Batch 2900, Loss: 0.1971\n",
      "  Batch 3000, Loss: 0.2175\n",
      "  Batch 3100, Loss: 0.1977\n",
      "  Batch 3200, Loss: 0.2067\n",
      "  Batch 3300, Loss: 0.2059\n",
      "  Batch 3400, Loss: 0.1970\n",
      "  Batch 3500, Loss: 0.1939\n",
      "  Batch 3600, Loss: 0.1978\n",
      "  Batch 3700, Loss: 0.1905\n",
      "  Batch 3800, Loss: 0.2025\n",
      "  Batch 3900, Loss: 0.2150\n",
      "  Batch 4000, Loss: 0.2078\n",
      "  Batch 4100, Loss: 0.2056\n",
      "  Batch 4200, Loss: 0.2061\n",
      "  Batch 4300, Loss: 0.2004\n",
      "  Batch 4400, Loss: 0.2173\n",
      "  Batch 4500, Loss: 0.2069\n",
      "  Batch 4600, Loss: 0.2091\n",
      "  Batch 4700, Loss: 0.2048\n",
      "  Batch 4800, Loss: 0.2091\n",
      "  Batch 4900, Loss: 0.2022\n",
      "  Batch 5000, Loss: 0.2034\n",
      "  Batch 5100, Loss: 0.2181\n",
      "  Batch 5200, Loss: 0.1999\n",
      "  Batch 5300, Loss: 0.2166\n",
      "  Batch 5400, Loss: 0.2134\n",
      "  Batch 5500, Loss: 0.2019\n",
      "  Batch 5600, Loss: 0.1992\n",
      "  Batch 5700, Loss: 0.1916\n",
      "  Batch 5800, Loss: 0.2071\n",
      "  Batch 5900, Loss: 0.2021\n",
      "  Batch 6000, Loss: 0.2092\n",
      "  Batch 6100, Loss: 0.2007\n",
      "  Batch 6200, Loss: 0.2026\n",
      "  Batch 6300, Loss: 0.1978\n",
      "  Batch 6400, Loss: 0.2066\n",
      "  Batch 6500, Loss: 0.2056\n",
      "  Batch 6600, Loss: 0.1941\n",
      "  Batch 6700, Loss: 0.1873\n",
      "  Batch 6800, Loss: 0.1968\n",
      "  Batch 6900, Loss: 0.1980\n",
      "  Batch 7000, Loss: 0.1951\n",
      "  Batch 7100, Loss: 0.2118\n",
      "  Batch 7200, Loss: 0.1972\n",
      "  Batch 7300, Loss: 0.2011\n",
      "Epoch 5 Average Loss: 0.2053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = execute_training(ratings, train_ratings, max_epochs=5, logging_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd650690",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T04:38:21.630779Z",
     "iopub.status.busy": "2024-11-30T04:38:21.630421Z",
     "iopub.status.idle": "2024-11-30T04:38:21.634709Z",
     "shell.execute_reply": "2024-11-30T04:38:21.633968Z"
    },
    "papermill": {
     "duration": 0.623742,
     "end_time": "2024-11-30T04:38:21.636330",
     "exception": false,
     "start_time": "2024-11-30T04:38:21.012588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processing_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58de054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T04:38:22.819283Z",
     "iopub.status.busy": "2024-11-30T04:38:22.818924Z",
     "iopub.status.idle": "2024-11-30T04:38:22.822883Z",
     "shell.execute_reply": "2024-11-30T04:38:22.822166Z"
    },
    "papermill": {
     "duration": 0.620985,
     "end_time": "2024-11-30T04:38:22.824537",
     "exception": false,
     "start_time": "2024-11-30T04:38:22.203552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a list of all movie IDs\n",
    "# all_movieIds = ratings['movieId'].unique()\n",
    "\n",
    "# # Placeholders that will hold the training data\n",
    "# users, items, labels = [], [], []\n",
    "\n",
    "# # This is the set of items that each user has interaction with\n",
    "# user_item_set = set(zip(train_ratings['userId'], train_ratings['movieId']))\n",
    "\n",
    "# # 4:1 ratio of negative to positive samples\n",
    "# num_negatives = 4\n",
    "\n",
    "# for (u, i) in tqdm(user_item_set):\n",
    "#     users.append(u)\n",
    "#     items.append(i)\n",
    "#     labels.append(1) # items that the user has interacted with are positive\n",
    "#     for _ in range(num_negatives):\n",
    "#         # randomly select an item\n",
    "#         negative_item = np.random.choice(all_movieIds) \n",
    "#         # check that the user has not interacted with this item\n",
    "#         while (u, negative_item) in user_item_set:\n",
    "#             negative_item = np.random.choice(all_movieIds)\n",
    "#         users.append(u)\n",
    "#         items.append(negative_item)\n",
    "#         labels.append(0) # items not interacted with are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "585b2f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T04:38:24.034605Z",
     "iopub.status.busy": "2024-11-30T04:38:24.033533Z",
     "iopub.status.idle": "2024-11-30T04:42:01.504460Z",
     "shell.execute_reply": "2024-11-30T04:42:01.503347Z"
    },
    "papermill": {
     "duration": 218.063151,
     "end_time": "2024-11-30T04:42:01.506417",
     "exception": false,
     "start_time": "2024-11-30T04:38:23.443266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4025/41547 [00:20<03:13, 194.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000: Current Hit Ratio @10 = 0.8203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 8022/41547 [00:41<02:52, 193.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8000: Current Hit Ratio @10 = 0.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 12029/41547 [01:01<02:31, 194.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12000: Current Hit Ratio @10 = 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 16030/41547 [01:22<02:09, 196.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16000: Current Hit Ratio @10 = 0.8230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 20022/41547 [01:43<01:52, 192.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20000: Current Hit Ratio @10 = 0.8229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 24038/41547 [02:04<01:30, 192.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 24000: Current Hit Ratio @10 = 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 28024/41547 [02:25<01:10, 191.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 28000: Current Hit Ratio @10 = 0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 32026/41547 [02:46<00:48, 195.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 32000: Current Hit Ratio @10 = 0.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 36033/41547 [03:07<00:28, 193.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36000: Current Hit Ratio @10 = 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 40031/41547 [03:28<00:08, 181.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 40000: Current Hit Ratio @10 = 0.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41547/41547 [03:35<00:00, 192.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hit Ratio @ 10 is 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# User-item pairs for testing\n",
    "test_user_item_set = set(zip(test_ratings['userId'], test_ratings['movieId']))\n",
    "\n",
    "# Dict of all items that are interacted with by each user\n",
    "user_interacted_items = ratings.groupby('userId')['movieId'].apply(list).to_dict()\n",
    "\n",
    "hits = []\n",
    "for step, (u, i) in enumerate(tqdm(test_user_item_set), start=1):\n",
    "    interacted_items = user_interacted_items[u]\n",
    "    not_interacted_items = set(all_movieIds) - set(interacted_items)\n",
    "    selected_not_interacted = list(np.random.choice(list(not_interacted_items), 99))\n",
    "    test_items = selected_not_interacted + [i]\n",
    "    \n",
    "    predicted_labels = np.squeeze(model(\n",
    "        torch.tensor([u] * 100).to('cuda'),\n",
    "        torch.tensor(test_items).to('cuda')\n",
    "    ).to('cpu').detach().numpy())\n",
    "    \n",
    "    top10_items = [test_items[j] for j in np.argsort(predicted_labels)[::-1][:10]]\n",
    "    \n",
    "    if i in top10_items:\n",
    "        hits.append(1)\n",
    "    else:\n",
    "        hits.append(0)\n",
    "    \n",
    "    # Print the hit ratio every 200 steps\n",
    "    if step % 4000 == 0:\n",
    "        current_hit_ratio = np.mean(hits)\n",
    "        print(f\"Step {step}: Current Hit Ratio @10 = {current_hit_ratio:.4f}\")\n",
    "\n",
    "# Final hit ratio\n",
    "print(\"The Hit Ratio @ 10 is {:.4f}\".format(np.mean(hits)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e428758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T04:42:02.941839Z",
     "iopub.status.busy": "2024-11-30T04:42:02.940981Z",
     "iopub.status.idle": "2024-11-30T04:42:02.962319Z",
     "shell.execute_reply": "2024-11-30T04:42:02.961419Z"
    },
    "papermill": {
     "duration": 0.741919,
     "end_time": "2024-11-30T04:42:02.964109",
     "exception": false,
     "start_time": "2024-11-30T04:42:02.222190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/kaggle/working/model_v3_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e266d56",
   "metadata": {
    "papermill": {
     "duration": 0.704708,
     "end_time": "2024-11-30T04:42:04.362798",
     "exception": false,
     "start_time": "2024-11-30T04:42:03.658090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 339,
     "sourceId": 77759,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5770.820134,
   "end_time": "2024-11-30T04:42:06.999914",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-30T03:05:56.179780",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
